# Machine Unlearning Literature Collection

A collection of methodologies, benchmarks, datasets, surverys and others related to Machine Unlearning.


## Surveys/General Machine Unlearning papers
| **Paper** | **Year** | 
| --------------- | ---- | 
| [A Survey of Machine Unlearning](https://arxiv.org/abs/2209.02299) | 2022 |
| [Machine Unlearning: A Survey](https://arxiv.org/abs/2306.03558) | 2023 |
| [Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges](https://arxiv.org/abs/2311.15766) | 2023 |
| [Right to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions](https://arxiv.org/abs/2307.03941) | 2023
| [Rethinking Machine Unlearning for Large Language Models](https://arxiv.org/abs/2402.08787) | 2024 |
| [Making Machine Learning Forget](https://www.semanticscholar.org/paper/Making-Machine-Learning-Forget-Shintre-Roundy/9e968ea013db548bd7640ba531eefdb41e2b7be3) | 2019 |
| [Unrolling SGD: Understanding Factors Influencing Machine Unlearning](https://arxiv.org/abs/2109.13398) | 2022 |
| [Making AI Forget You: Data Deletion in Machine Learning]() | 2019 |
| [DeltaGrad: Rapid retraining of machine learning models]() | 2020 |
| [Machine Unlearning]() | 2020 |
| [Adaptive Machine Unlearning]() | 2021 |
| [Approximate Data Deletion from Machine Learning Models]() | 2021 |
| [Remember What You Want to Forget: Algorithms for Machine Unlearning]() | 2021 |
| [ARCANE: An Efficient Architecture for Exact Machine Unlearning]() | 2022 |
| [Continual Learning and Private Unlearning]() | 2022 |
| [Learning with Recoverable Forgetting]() | 2022 |
| [On the Necessity of Auditable Algorithmic Definitions for Machine Unlearning]() | 2022 |
| [Ticketed Learning–Unlearning Schemes]() | 2023 |
| [Machine Unlearning of Features and Labels]() | |
| [Corrective Machine Unlearning]() | 2024 | 
| [Certified Data Removal from Machine Learning Models]() | 2023 | 
| [Machine Unlearning Fails to Remove Data Poisoning Attacks]() | 2024 | 
| [Towards Making Systems Forget with Machine Unlearning]() | 2015 | 
| [Extracting Training Data from Large Language Models]() | 2021 | 
| [Are Large Pre-Trained Language Models Leaking Your Personal Information?]() | 2022 | 
| [QUANTIFYING MEMORIZATION ACROSS NEURAL LANGUAGE MODELS]() | 2023 | 
| [Unlearning Bias in Language Models by Partitioning Gradients]() | 2023 | 
| [LLM Agents can Autonomously Hack Websites]() | 2024 | 
| [Artificial intelligence and biological misuse: Differentiating risks of language models and biological design tools]() | 2023 | 
| [Machine Unlearning Fails to Remove Data Poisoning Attacks]() | 2024 |
| [Large Language Models Relearn Removed Concepts]() | 2024 | 
----------

## Methodologies
| **Paper** | **Year** | 
| --------------- | ---- | 
| [Nuanced Metrics for Measuring Unintended Bias with Real Data for Text Classification]() | 2019 |
| [Eternal Sunshine of the Spotless Net: Selective Forgetting in Deep Networks]() | 2020 | 
| [Federated Unlearning]() | 2021 | 
| [Making Recommender Systems Forget: Learning and Unlearning for Erasable Recommendation]() | 2022 | 
| [Can Bad Teaching Induce Forgetting? Unlearning in Deep Networks Using an Incompetent Teacher]() | 2023 | 
| [Erasing Concepts from Diffusion Models]() | 2023 | 
| [Fast Federated Machine Unlearning with Nonlinear Functional Theory]() | 2023 | 
| [Forget-Me-Not: Learning to Forget in Text-to-Image Diffusion Models]() | 2023 | 
| [Forget Unlearning: Towards True Data-Deletion in Machine Learning]() | 2023 | 
| [Towards Unbounded Machine Unlearning]() | 2023 | 
| [Fast Machine Unlearning Without Retraining Through Selective Synaptic Dampening]() | 2023 | 
| [EDITING MODELS WITH TASK ARITHMETIC]() | 2023 | 
| [Who’s Harry Potter? Approximate Unlearning in LLMs]() | 2023 | 
| [In-Context Unlearning: Language Models as Few-Shot Unlearners]() | 2024 | 
| [Large Language Model Unlearning]() | 2024 | 
| [Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning]() | 2024 | 
| [SALUN: EMPOWERING MACHINE UNLEARNING VIA GRADIENT- BASED WEIGHT SALIENCY IN BOTH IMAGE CLASSIFICATION AND GENERATION]() | 2024 | 
| [The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning]() | 2024 | 
| [Single Image Unlearning: Efficient Machine Unlearning in Multimodal Large Language Models]() | 2024 | 
| [Descent-to-Delete: Gradient-Based Methods for Machine Unlearning](https://arxiv.org/abs/2007.02923) | 2020 | 
| [Langevin Unlearning: A New Perspective of Noisy Gradient Descent for Machine Unlearning](https://arxiv.org/abs/2401.10371) | 2024 | 
| [Amnesiac Machine Learning](https://arxiv.org/abs/2010.10981) | 2021 | 
| [Knowledge Unlearning for Mitigating Privacy Risks in Language Models](https://arxiv.org/abs/2210.01504) | 2022 | 
| [Towards Adversarial Evaluations for Inexact Machine Unlearning](https://arxiv.org/abs/2201.06640) | 2022 | 
| [Continual Learning and Private Unlearning]() | 2022 |
| [TOFU: A Task of Fictitious Unlearning for LLMs](https://arxiv.org/abs/2401.06121) | 2024 | 
| [In-Context Unlearning: Language Models as Few-Shot Unlearners]() | 2024 |
| [Zero-Shot Machine Unlearning](https://arxiv.org/abs/2201.05629) | 2022 | 
| [Self-Destructing Models: Increasing the Costs of Harmful Dual Uses of Foundation Models]() | 2023 | 
| [FINE - TUNING ALIGNED LANGUAGE MODELS COMPROMISES SAFETY, EVEN WHEN USERS DO NOT INTEND TO!]() | 2023 | 
| [LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B](https://arxiv.org/abs/2310.20624) | 2023 | 
| [Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models](https://arxiv.org/abs/2310.02949) | 2023 | 
| [Removing RLHF Protections in GPT-4 via Fine-Tuning]() | 2024 | 

----------

## Evaluation, Benchmarks and Datasets
| **Paper** | **Year** | 
| --------------- | ---- | 
| [TOFU: A Task of Fictitious Unlearning for LLMs](https://arxiv.org/abs/2401.06121) | 2024 | 
| [EIGHT METHODS TO EVALUATE ROBUST UNLEARNING IN LLMS]() | 2024 | 
| [MUSE: Machine Unlearning Six-Way Evaluation for Language Models]() | 2024 |
| [The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning]() | 2024 |
| [BEAVERTAILS: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset]() | 2023 |
| [DETECTING PRETRAINING DATA FROM LARGE LANGUAGE M ODELS]() | 2024 | 
| [Enhanced Membership Inference Attacks against Machine Learning Models]() | 2022 | 
| [Quantifying the Privacy Risks of Learning High-Dimensional Graphical Models]() | 2021 | 
| [Towards Probabilistic Verification of Machine Unlearning]() | 2020 | 
| [Have you forgotten? A method to assess if machine learning models have forgotten data]() | 2020 | 
| [Privacy Adhering Machine Un-learning in NLP]() | 2021 | 
| [Quark: Controllable Text Generation with Reinforced Unlearning]() | 2022 | 
| [CAN SENSITIVE INFORMATION BE DELETED FROM LLMS? OBJECTIVES FOR DEFENDING AGAINST EXTRACTION ATTACKS]() | 2023 | 
| [Large Language Models Relearn Removed Concepts]() | 2024 | 
| [Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space]() | 2024 | 
| [Unlearn What You Want to Forget: Efficient Unlearning for LLMs]() | 2023 | 
| [KGA: A General Machine Unlearning Framework Based on Knowledge Gap Alignment]() | 2023 | 
----------



















